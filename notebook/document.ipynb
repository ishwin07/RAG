{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d1080b9",
   "metadata": {},
   "source": [
    "### Data Ingestion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40aff8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### document structure\n",
    "\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65aebce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'user_manual.pdf', 'pages': 1, 'author': 'John Doe'}, page_content='This is the content of the document.')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=Document(\n",
    "    page_content=\"This is the content of the document.\", \n",
    "    metadata={\n",
    "        \"source\": \"user_manual.pdf\",\n",
    "        \"pages\":1,\n",
    "        \"author\":\"John Doe\"}\n",
    "    )\n",
    "\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcb7a7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a simple txt file\n",
    "\n",
    "import os\n",
    "os.makedirs(\"../data/text_files\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d166181",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_texts={\n",
    "    \"../data/text_files/doc1.txt\": \"\"\"RAG Intoduction\n",
    "    Retrieval-Augmented Generation (RAG) is a technique used to enhance large language models by combining text generation with information retrieval. Instead of relying only on the knowledge stored inside the model during training, RAG systems dynamically fetch relevant information from external data sources before generating a response.\n",
    "\n",
    "In a typical RAG pipeline, documents such as PDFs, text files, or web pages are first loaded and converted into a structured format. These documents are then split into smaller chunks to ensure efficient processing. Each chunk is transformed into a numerical representation called an embedding, which captures the semantic meaning of the text.\n",
    "\n",
    "When a user asks a question, the query is also converted into an embedding and compared against the stored document embeddings using a vector similarity search. The most relevant chunks are retrieved and passed to a language model as additional context. This allows the model to produce more accurate, up-to-date, and context-aware answers.\n",
    "\n",
    "RAG is especially useful in applications like chatbots, question-answering systems, and enterprise knowledge assistants, where factual accuracy and domain-specific information are critical. By grounding model responses in retrieved documents, RAG helps reduce hallucinations and improves trustworthiness.\"\"\"\n",
    "\n",
    "}\n",
    "\n",
    "for file_path, content in sample_texts.items():\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "179fb548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../data/text_files/doc1.txt'}, page_content='RAG Intoduction\\n    Retrieval-Augmented Generation (RAG) is a technique used to enhance large language models by combining text generation with information retrieval. Instead of relying only on the knowledge stored inside the model during training, RAG systems dynamically fetch relevant information from external data sources before generating a response.\\n\\nIn a typical RAG pipeline, documents such as PDFs, text files, or web pages are first loaded and converted into a structured format. These documents are then split into smaller chunks to ensure efficient processing. Each chunk is transformed into a numerical representation called an embedding, which captures the semantic meaning of the text.\\n\\nWhen a user asks a question, the query is also converted into an embedding and compared against the stored document embeddings using a vector similarity search. The most relevant chunks are retrieved and passed to a language model as additional context. This allows the model to produce more accurate, up-to-date, and context-aware answers.\\n\\nRAG is especially useful in applications like chatbots, question-answering systems, and enterprise knowledge assistants, where factual accuracy and domain-specific information are critical. By grounding model responses in retrieved documents, RAG helps reduce hallucinations and improves trustworthiness.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##TextLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader= TextLoader(\"../data/text_files/doc1.txt\", encoding=\"utf-8\")\n",
    "documents=loader.load()\n",
    "documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db4c8dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '..\\\\data\\\\text_files\\\\doc1.txt'}, page_content='RAG Intoduction\\n    Retrieval-Augmented Generation (RAG) is a technique used to enhance large language models by combining text generation with information retrieval. Instead of relying only on the knowledge stored inside the model during training, RAG systems dynamically fetch relevant information from external data sources before generating a response.\\n\\nIn a typical RAG pipeline, documents such as PDFs, text files, or web pages are first loaded and converted into a structured format. These documents are then split into smaller chunks to ensure efficient processing. Each chunk is transformed into a numerical representation called an embedding, which captures the semantic meaning of the text.\\n\\nWhen a user asks a question, the query is also converted into an embedding and compared against the stored document embeddings using a vector similarity search. The most relevant chunks are retrieved and passed to a language model as additional context. This allows the model to produce more accurate, up-to-date, and context-aware answers.\\n\\nRAG is especially useful in applications like chatbots, question-answering systems, and enterprise knowledge assistants, where factual accuracy and domain-specific information are critical. By grounding model responses in retrieved documents, RAG helps reduce hallucinations and improves trustworthiness.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## directory loader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "dir_loader= DirectoryLoader(\"../data/text_files\", \n",
    "glob=\"*.txt\",  #Pattern to match files\n",
    "loader_cls=TextLoader, #Loader class to use\n",
    "loader_kwargs={\"encoding\":\"utf-8\"},#Loader specific arguments\n",
    "show_progress=False\n",
    ")\n",
    "\n",
    "documents=dir_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee4cdaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF files created successfully.\n"
     ]
    }
   ],
   "source": [
    "from reportlab.lib.pagesizes import LETTER\n",
    "from reportlab.pdfgen import canvas\n",
    "import os\n",
    "\n",
    "# Ensure folder exists\n",
    "os.makedirs(\"../data/pdf_files\", exist_ok=True)\n",
    "\n",
    "# ---------- PDF 1 ----------\n",
    "pdf1_path = \"../data/pdf_files/rag_intro.pdf\"\n",
    "c1 = canvas.Canvas(pdf1_path, pagesize=LETTER)\n",
    "\n",
    "c1.drawString(72, 750, \"Introduction to Retrieval-Augmented Generation (RAG)\")\n",
    "c1.drawString(72, 720, \"RAG combines information retrieval with text generation.\")\n",
    "c1.drawString(72, 690, \"It improves factual accuracy by grounding LLMs in documents.\")\n",
    "\n",
    "c1.save()\n",
    "\n",
    "# ---------- PDF 2 ----------\n",
    "pdf2_path = \"../data/pdf_files/rag_pipeline.pdf\"\n",
    "c2 = canvas.Canvas(pdf2_path, pagesize=LETTER)\n",
    "\n",
    "c2.drawString(72, 750, \"RAG Pipeline Overview\")\n",
    "c2.drawString(72, 720, \"1. Load documents (PDFs, text files, etc.)\")\n",
    "c2.drawString(72, 690, \"2. Chunk documents\")\n",
    "c2.drawString(72, 660, \"3. Generate embeddings\")\n",
    "c2.drawString(72, 630, \"4. Retrieve relevant chunks for a query\")\n",
    "\n",
    "c2.save()\n",
    "\n",
    "print(\"PDF files created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9395152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': 'anonymous', 'creationdate': '2026-01-24T10:59:30+05:00', 'source': '..\\\\data\\\\pdf_files\\\\rag_intro.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\rag_intro.pdf', 'total_pages': 1, 'format': 'PDF 1.3', 'title': 'untitled', 'author': 'anonymous', 'subject': 'unspecified', 'keywords': '', 'moddate': '2026-01-24T10:59:30+05:00', 'trapped': '', 'modDate': \"D:20260124105930+05'00'\", 'creationDate': \"D:20260124105930+05'00'\", 'page': 0}, page_content='Introduction to Retrieval-Augmented Generation (RAG)\\nRAG combines information retrieval with text generation.\\nIt improves factual accuracy by grounding LLMs in documents.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': 'anonymous', 'creationdate': '2026-01-24T10:59:30+05:00', 'source': '..\\\\data\\\\pdf_files\\\\rag_pipeline.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\rag_pipeline.pdf', 'total_pages': 1, 'format': 'PDF 1.3', 'title': 'untitled', 'author': 'anonymous', 'subject': 'unspecified', 'keywords': '', 'moddate': '2026-01-24T10:59:30+05:00', 'trapped': '', 'modDate': \"D:20260124105930+05'00'\", 'creationDate': \"D:20260124105930+05'00'\", 'page': 0}, page_content='RAG Pipeline Overview\\n1. Load documents (PDFs, text files, etc.)\\n2. Chunk documents\\n3. Generate embeddings\\n4. Retrieve relevant chunks for a query')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader,PyMuPDFLoader\n",
    "\n",
    "dir_loader= DirectoryLoader(\n",
    "    \"../data/pdf_files\", \n",
    "    glob=\"*.pdf\",  #Pattern to match files\n",
    "    loader_cls=PyMuPDFLoader, #Loader class to use\n",
    "    #loader_cls=PyPDFLoader,\n",
    "   \n",
    "    show_progress=False\n",
    ")\n",
    "\n",
    "pdf_documents=dir_loader.load()\n",
    "pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c0bed4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "print(type(pdf_documents))\n",
    "print(type(pdf_documents[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a80d29",
   "metadata": {},
   "source": [
    "### embedding and vectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b78bd971",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OM\\Desktop\\RAG\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any,Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f81e13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OM\\Desktop\\RAG\\.venv\\Scripts\\python.exe: No module named pip\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6be9db94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully.Embedding Dimension:384 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x1337a6edc90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\" Handeles embedding generation using SentenceTransformer models. \"\"\"\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager with a specified SentenceTransformer model.\n",
    "        Args:\n",
    "            model_name (str): HuggingFace Model name for sentence embeddings.\n",
    "\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\" Load the SentenceTransformer model. \"\"\"\n",
    "        try:\n",
    "            print(f\"Loading Embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully.Embedding Dimension:{self.model.get_sentence_embedding_dimension()} \")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            raise e\n",
    "        \n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts.\n",
    "        Args:\n",
    "            texts (List[str]): List of input texts to embed.\n",
    "\n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (len(texts), embedding_dimension)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Embedding model is not loaded.\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts....\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "\n",
    "\n",
    "    ### initializing the embedding manager\n",
    "embedding_manager= EmbeddingManager()\n",
    "embedding_manager\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c436ed41",
   "metadata": {},
   "source": [
    "### VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e5e6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    \"\"\" Manages storage and retrieval of document embeddings using ChromaDB. \"\"\"\n",
    "    def __init__(self, collection_name: str = \"pdf_documents\",persist_directory: str=\"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "        Initialize the vector store with a specified collection name.\n",
    "        Args:\n",
    "            collection_name (str): Name of the ChromaDB collection.\n",
    "            persist_directory (str): Directory to persist vector store.\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "\n",
    "    def _initialize_chromadb(self):\n",
    "        \"\"\" Initialize ChromaDB client and collection. \"\"\"\n",
    "        try:\n",
    "            print(\"Initializing ChromaDB client...\")\n",
    "            self.client = chromadb.Client(Settings(\n",
    "                chroma_db_impl=\"duckdb+parquet\",\n",
    "                persist_directory=\"./chroma_db_data\"\n",
    "            ))\n",
    "            self.collection = self.client.get_or_create_collection(name=self.collection_name)\n",
    "            print(f\"ChromaDB collection '{self.collection_name}' is ready.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing ChromaDB: {e}\")\n",
    "            raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
