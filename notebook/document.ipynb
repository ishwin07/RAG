{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d1080b9",
   "metadata": {},
   "source": [
    "### Data Ingestion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40aff8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### document structure\n",
    "\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65aebce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'user_manual.pdf', 'pages': 1, 'author': 'John Doe'}, page_content='This is the content of the document.')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=Document(\n",
    "    page_content=\"This is the content of the document.\", \n",
    "    metadata={\n",
    "        \"source\": \"user_manual.pdf\",\n",
    "        \"pages\":1,\n",
    "        \"author\":\"John Doe\"}\n",
    "    )\n",
    "\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcb7a7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a simple txt file\n",
    "\n",
    "import os\n",
    "os.makedirs(\"../data/text_files\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d166181",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_texts={\n",
    "    \"../data/text_files/doc1.txt\": \"\"\"RAG Intoduction\n",
    "    Retrieval-Augmented Generation (RAG) is a technique used to enhance large language models by combining text generation with information retrieval. Instead of relying only on the knowledge stored inside the model during training, RAG systems dynamically fetch relevant information from external data sources before generating a response.\n",
    "\n",
    "In a typical RAG pipeline, documents such as PDFs, text files, or web pages are first loaded and converted into a structured format. These documents are then split into smaller chunks to ensure efficient processing. Each chunk is transformed into a numerical representation called an embedding, which captures the semantic meaning of the text.\n",
    "\n",
    "When a user asks a question, the query is also converted into an embedding and compared against the stored document embeddings using a vector similarity search. The most relevant chunks are retrieved and passed to a language model as additional context. This allows the model to produce more accurate, up-to-date, and context-aware answers.\n",
    "\n",
    "RAG is especially useful in applications like chatbots, question-answering systems, and enterprise knowledge assistants, where factual accuracy and domain-specific information are critical. By grounding model responses in retrieved documents, RAG helps reduce hallucinations and improves trustworthiness.\"\"\"\n",
    "\n",
    "}\n",
    "\n",
    "for file_path, content in sample_texts.items():\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "179fb548",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OM\\Desktop\\RAG\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../data/text_files/doc1.txt'}, page_content='RAG Intoduction\\n    Retrieval-Augmented Generation (RAG) is a technique used to enhance large language models by combining text generation with information retrieval. Instead of relying only on the knowledge stored inside the model during training, RAG systems dynamically fetch relevant information from external data sources before generating a response.\\n\\nIn a typical RAG pipeline, documents such as PDFs, text files, or web pages are first loaded and converted into a structured format. These documents are then split into smaller chunks to ensure efficient processing. Each chunk is transformed into a numerical representation called an embedding, which captures the semantic meaning of the text.\\n\\nWhen a user asks a question, the query is also converted into an embedding and compared against the stored document embeddings using a vector similarity search. The most relevant chunks are retrieved and passed to a language model as additional context. This allows the model to produce more accurate, up-to-date, and context-aware answers.\\n\\nRAG is especially useful in applications like chatbots, question-answering systems, and enterprise knowledge assistants, where factual accuracy and domain-specific information are critical. By grounding model responses in retrieved documents, RAG helps reduce hallucinations and improves trustworthiness.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##TextLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader= TextLoader(\"../data/text_files/doc1.txt\", encoding=\"utf-8\")\n",
    "documents=loader.load()\n",
    "documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db4c8dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '..\\\\data\\\\text_files\\\\doc1.txt'}, page_content='RAG Intoduction\\n    Retrieval-Augmented Generation (RAG) is a technique used to enhance large language models by combining text generation with information retrieval. Instead of relying only on the knowledge stored inside the model during training, RAG systems dynamically fetch relevant information from external data sources before generating a response.\\n\\nIn a typical RAG pipeline, documents such as PDFs, text files, or web pages are first loaded and converted into a structured format. These documents are then split into smaller chunks to ensure efficient processing. Each chunk is transformed into a numerical representation called an embedding, which captures the semantic meaning of the text.\\n\\nWhen a user asks a question, the query is also converted into an embedding and compared against the stored document embeddings using a vector similarity search. The most relevant chunks are retrieved and passed to a language model as additional context. This allows the model to produce more accurate, up-to-date, and context-aware answers.\\n\\nRAG is especially useful in applications like chatbots, question-answering systems, and enterprise knowledge assistants, where factual accuracy and domain-specific information are critical. By grounding model responses in retrieved documents, RAG helps reduce hallucinations and improves trustworthiness.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## directory loader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "dir_loader= DirectoryLoader(\"../data/text_files\", \n",
    "glob=\"*.txt\",  #Pattern to match files\n",
    "loader_cls=TextLoader, #Loader class to use\n",
    "loader_kwargs={\"encoding\":\"utf-8\"},#Loader specific arguments\n",
    "show_progress=False\n",
    ")\n",
    "\n",
    "documents=dir_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee4cdaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF files created successfully.\n"
     ]
    }
   ],
   "source": [
    "from reportlab.lib.pagesizes import LETTER\n",
    "from reportlab.pdfgen import canvas\n",
    "import os\n",
    "\n",
    "# Ensure folder exists\n",
    "os.makedirs(\"../data/pdf_files\", exist_ok=True)\n",
    "\n",
    "# ---------- PDF 1 ----------\n",
    "pdf1_path = \"../data/pdf_files/rag_intro.pdf\"\n",
    "c1 = canvas.Canvas(pdf1_path, pagesize=LETTER)\n",
    "\n",
    "c1.drawString(72, 750, \"Introduction to Retrieval-Augmented Generation (RAG)\")\n",
    "c1.drawString(72, 720, \"RAG combines information retrieval with text generation.\")\n",
    "c1.drawString(72, 690, \"It improves factual accuracy by grounding LLMs in documents.\")\n",
    "\n",
    "c1.save()\n",
    "\n",
    "# ---------- PDF 2 ----------\n",
    "pdf2_path = \"../data/pdf_files/rag_pipeline.pdf\"\n",
    "c2 = canvas.Canvas(pdf2_path, pagesize=LETTER)\n",
    "\n",
    "c2.drawString(72, 750, \"RAG Pipeline Overview\")\n",
    "c2.drawString(72, 720, \"1. Load documents (PDFs, text files, etc.)\")\n",
    "c2.drawString(72, 690, \"2. Chunk documents\")\n",
    "c2.drawString(72, 660, \"3. Generate embeddings\")\n",
    "c2.drawString(72, 630, \"4. Retrieve relevant chunks for a query\")\n",
    "\n",
    "c2.save()\n",
    "\n",
    "print(\"PDF files created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9395152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': 'anonymous', 'creationdate': '2026-01-31T14:59:56+05:00', 'source': '..\\\\data\\\\pdf_files\\\\rag_intro.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\rag_intro.pdf', 'total_pages': 1, 'format': 'PDF 1.3', 'title': 'untitled', 'author': 'anonymous', 'subject': 'unspecified', 'keywords': '', 'moddate': '2026-01-31T14:59:56+05:00', 'trapped': '', 'modDate': \"D:20260131145956+05'00'\", 'creationDate': \"D:20260131145956+05'00'\", 'page': 0}, page_content='Introduction to Retrieval-Augmented Generation (RAG)\\nRAG combines information retrieval with text generation.\\nIt improves factual accuracy by grounding LLMs in documents.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': 'anonymous', 'creationdate': '2026-01-31T14:59:56+05:00', 'source': '..\\\\data\\\\pdf_files\\\\rag_pipeline.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\rag_pipeline.pdf', 'total_pages': 1, 'format': 'PDF 1.3', 'title': 'untitled', 'author': 'anonymous', 'subject': 'unspecified', 'keywords': '', 'moddate': '2026-01-31T14:59:56+05:00', 'trapped': '', 'modDate': \"D:20260131145956+05'00'\", 'creationDate': \"D:20260131145956+05'00'\", 'page': 0}, page_content='RAG Pipeline Overview\\n1. Load documents (PDFs, text files, etc.)\\n2. Chunk documents\\n3. Generate embeddings\\n4. Retrieve relevant chunks for a query')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader,PyMuPDFLoader\n",
    "\n",
    "dir_loader= DirectoryLoader(\n",
    "    \"../data/pdf_files\", \n",
    "    glob=\"*.pdf\",  #Pattern to match files\n",
    "    loader_cls=PyMuPDFLoader, #Loader class to use\n",
    "    #loader_cls=PyPDFLoader,\n",
    "   \n",
    "    show_progress=False\n",
    ")\n",
    "\n",
    "pdf_documents=dir_loader.load()\n",
    "pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c0bed4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "print(type(pdf_documents))\n",
    "print(type(pdf_documents[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a80d29",
   "metadata": {},
   "source": [
    "### embedding and vectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b78bd971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any,Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f81e13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\om\\desktop\\rag\\.venv\\lib\\site-packages (5.2.0)\n",
      "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in c:\\users\\om\\desktop\\rag\\.venv\\lib\\site-packages (from sentence-transformers) (4.57.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\om\\desktop\\rag\\.venv\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\om\\desktop\\rag\\.venv\\lib\\site-packages (from sentence-transformers) (2.10.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\om\\desktop\\rag\\.venv\\lib\\site-packages (from sentence-transformers) (1.8.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\om\\desktop\\rag\\.venv\\lib\\site-packages (from sentence-transformers) (1.17.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\om\\desktop\\rag\\.venv\\lib\\site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\om\\desktop\\rag\\.venv\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\om\\desktop\\rag\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\om\\desktop\\rag\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2026.1.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\om\\desktop\\rag\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\om\\desktop\\rag\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: requests in c:\\users\\om\\desktop\\rag\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\om\\desktop\\rag\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\om\\desktop\\rag\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\om\\desktop\\rag\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\om\\desktop\\rag\\.venv\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\om\\desktop\\rag\\.venv\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\om\\desktop\\rag\\.venv\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2026.1.15)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\om\\desktop\\rag\\.venv\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\om\\desktop\\rag\\.venv\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: joblib>=1.3.0 in c:\\users\\om\\desktop\\rag\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in c:\\users\\om\\desktop\\rag\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\om\\desktop\\rag\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\om\\desktop\\rag\\.venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\om\\desktop\\rag\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\om\\desktop\\rag\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\om\\desktop\\rag\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\om\\desktop\\rag\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2026.1.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 26.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6be9db94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Any\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "\n",
    "class EmbeddingManager:\n",
    "    \"\"\"Handles chunking and embedding generation using SentenceTransformer models.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"all-MiniLM-L6-v2\",\n",
    "        chunk_size: int = 1000,\n",
    "        chunk_overlap: int = 200\n",
    "    ):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model.\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(\n",
    "                f\"Model loaded successfully. \"\n",
    "                f\"Embedding dimension: {self.model.get_sentence_embedding_dimension()}\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def chunk_documents(self, documents: List[Any]) -> List[Any]:\n",
    "        \"\"\"Split documents into overlapping chunks.\"\"\"\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=self.chunk_overlap,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "\n",
    "        chunks = splitter.split_documents(documents)\n",
    "        print(f\"[INFO] Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "        return chunks\n",
    "\n",
    "    def embed_chunks(self, chunks: List[Any]) -> np.ndarray:\n",
    "        \"\"\"Generate embeddings for document chunks.\"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Embedding model is not loaded.\")\n",
    "\n",
    "        texts = [chunk.page_content for chunk in chunks]\n",
    "        print(f\"[INFO] Generating embeddings for {len(texts)} chunks...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"[INFO] Embeddings shape: {embeddings.shape}\")\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c436ed41",
   "metadata": {},
   "source": [
    "### VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0e5e6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ChromaDB client...\n",
      "ChromaDB collection 'pdf_documents' is ready.\n",
      "Existing documents in collection: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x1f5bf95c8d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "class VectorStore:\n",
    "    \"\"\" Manages storage and retrieval of document embeddings using ChromaDB. \"\"\"\n",
    "    def __init__(self, collection_name: str = \"pdf_documents\",persist_directory: str=\"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "        Initialize the vector store with a specified collection name.\n",
    "        Args:\n",
    "            collection_name (str): Name of the ChromaDB collection.\n",
    "            persist_directory (str): Directory to persist vector store.\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\" Initialize ChromaDB client and collection. \"\"\"\n",
    "        try:\n",
    "            #Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            print(\"Initializing ChromaDB client...\")\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory )\n",
    "\n",
    "            self.collection = self.client.get_or_create_collection(name=self.collection_name,metadata={\"description\":\"PDF Document embeddings for RAG\"})\n",
    "            print(f\"ChromaDB collection '{self.collection_name}' is ready.\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing ChromaDB: {e}\")\n",
    "            raise e\n",
    "        \n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store.\n",
    "        Args:\n",
    "            documents (List[Any]): List of Langchain Documents.\n",
    "            embeddings (np.ndarray): Corresponding embeddings for the documents.\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents and embeddings must match.\")\n",
    "       \n",
    "\n",
    "        print(f\"Adding {len(documents)} documents to the vector store...\")\n",
    "\n",
    "        #Prepare data for ChromaDB\n",
    "        ids=[]\n",
    "        metadatas=[]\n",
    "        documents_text=[]\n",
    "        embeddings_list=[]\n",
    "\n",
    "        for i,(doc,embedding) in enumerate(zip(documents,embeddings)):\n",
    "            #Generate unique ID for each document\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            #Prepare metadata and content\n",
    "            metadata=dict(doc.metadata)  #Copy metadata\n",
    "            metadata['chunk_index'] = i\n",
    "            metadata['content_length']= len(doc.page_content)\n",
    "            metadata['doc_type'] = \"text\" \n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            #Document content and embedding\n",
    "            documents_text.append(doc.page_content)\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "\n",
    "         #Add to ChromaDB collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "            ids=ids,\n",
    "            metadatas=metadatas,\n",
    "            documents=documents_text,\n",
    "            embeddings=embeddings_list\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to the vector store.\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise e\n",
    "    \n",
    "\n",
    "### Initializing the vector store\n",
    "vector_store= VectorStore()\n",
    "vector_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2103796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimension: 384\n",
      "[INFO] Split 1 documents into 2 chunks.\n",
      "Total chunks: 2\n",
      "\n",
      "--- First chunk ---\n",
      "\n",
      "RAG Intoduction\n",
      "    Retrieval-Augmented Generation (RAG) is a technique used to enhance large language models by combining text generation with information retrieval. Instead of relying only on the knowledge stored inside the model during training, RAG systems dynamically fetch relevant information from external data sources before generating a response.\n",
      "\n",
      "In a typical RAG pipeline, documents such as PDFs, text files, or web pages are first loaded and converted into a structured format. These documents are then split into smaller chunks to ensure efficient processing. Each chunk is transformed into a numerical representation called an embedding, which captures the semantic meaning of the text.\n",
      "\n",
      "Metadata: {'source': '..\\\\data\\\\text_files\\\\doc1.txt'}\n"
     ]
    }
   ],
   "source": [
    "embedding_manager=EmbeddingManager()\n",
    "chunks = embedding_manager.chunk_documents(documents)\n",
    "\n",
    "print(\"Total chunks:\", len(chunks))\n",
    "print(\"\\n--- First chunk ---\\n\")\n",
    "print(chunks[0].page_content)\n",
    "print(\"\\nMetadata:\", chunks[0].metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb1dcbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1 length: 701\n",
      "Chunk 2 length: 646\n"
     ]
    }
   ],
   "source": [
    "for i, chunk in enumerate(chunks[:5]):\n",
    "    print(f\"Chunk {i+1} length:\", len(chunk.page_content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f8fc2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ChromaDB client...\n",
      "ChromaDB collection 'pdf_documents' is ready.\n",
      "Existing documents in collection: 2\n",
      "[INFO] Split 1 documents into 2 chunks.\n",
      "\n",
      "--- SAMPLE CHUNK ---\n",
      "\n",
      "RAG Intoduction\n",
      "    Retrieval-Augmented Generation (RAG) is a technique used to enhance large language models by combining text generation with information retrieval. Instead of relying only on the knowledge stored inside the model during training, RAG systems dynamically fetch relevant information from external data sources before generating a response.\n",
      "\n",
      "In a typical RAG pipeline, documents such as PDFs, text files, or web pages are first loaded and converted into a structured format. These doc\n",
      "\n",
      "Metadata: {'source': '..\\\\data\\\\text_files\\\\doc1.txt'}\n",
      "[INFO] Generating embeddings for 2 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Embeddings shape: (2, 384)\n",
      "Adding 2 documents to the vector store...\n",
      "Successfully added 2 documents to the vector store.\n",
      "Total documents in collection: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize managers\n",
    "\n",
    "vector_store = VectorStore()\n",
    "\n",
    "# 1️⃣ Chunk documents\n",
    "chunks = embedding_manager.chunk_documents(documents)\n",
    "\n",
    "# (Optional) See a chunk\n",
    "print(\"\\n--- SAMPLE CHUNK ---\\n\")\n",
    "print(chunks[0].page_content[:500])\n",
    "print(\"\\nMetadata:\", chunks[0].metadata)\n",
    "\n",
    "# 2️⃣ Generate embeddings\n",
    "embeddings = embedding_manager.embed_chunks(chunks)\n",
    "\n",
    "# 3️⃣ Store in ChromaDB\n",
    "vector_store.add_documents(chunks, embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481b0a46",
   "metadata": {},
   "source": [
    "### Retriever Pipeline from VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96a73b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles retrieval of relevant document chunks for a given query.\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Initialize the retriever with vector store and embedding manager.\n",
    "        \n",
    "        Args:\n",
    "            vector_store (VectorStore): The vector store instance.  \n",
    "            embedding_manager (EmbeddingManager): The embedding manager instance.\n",
    "\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "       \n",
    "\n",
    "    def retrieve(self, query: str,top_k:5,score_threshold:float=0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve top-k relevant document chunks for the query.\n",
    "        \n",
    "        Args:\n",
    "            query (str): The user query.\n",
    "            top_k (int): Number of top relevant chunks to retrieve.\n",
    "            score_threshold (float): Minimum similarity score threshold.\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict[str, Any]]: List of retrieved document chunks with metadata.\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score Threshold: {score_threshold}\")\n",
    "        \n",
    "        #Generate embedding for the query\n",
    "        query_embedding = self.embedding_manager.model.encode([query])[0]\n",
    "\n",
    "        #Perform similarity search in the vector store\n",
    "        results = self.vector_store.collection.query(\n",
    "            query_embeddings=[query_embedding.tolist()],\n",
    "            n_results=self.top_k\n",
    "        )\n",
    "\n",
    "        #Format results\n",
    "        retrieved_chunks = []\n",
    "        for doc_id, doc_content, metadata in zip(results['ids'][0], results['documents'][0], results['metadatas'][0]):\n",
    "            retrieved_chunks.append({\n",
    "                \"id\": doc_id,\n",
    "                \"content\": doc_content,\n",
    "                \"metadata\": metadata\n",
    "            })\n",
    "\n",
    "        return retrieved_chunks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
