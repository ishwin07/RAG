{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d1080b9",
   "metadata": {},
   "source": [
    "### Data Ingestion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40aff8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### document structure\n",
    "\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65aebce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'user_manual.pdf', 'pages': 1, 'author': 'John Doe'}, page_content='This is the content of the document.')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=Document(\n",
    "    page_content=\"This is the content of the document.\", \n",
    "    metadata={\n",
    "        \"source\": \"user_manual.pdf\",\n",
    "        \"pages\":1,\n",
    "        \"author\":\"John Doe\"}\n",
    "    )\n",
    "\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcb7a7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a simple txt file\n",
    "\n",
    "import os\n",
    "os.makedirs(\"../data/text_files\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d166181",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_texts={\n",
    "    \"../data/text_files/doc1.txt\": \"\"\"RAG Intoduction\n",
    "    Retrieval-Augmented Generation (RAG) is a technique used to enhance large language models by combining text generation with information retrieval. Instead of relying only on the knowledge stored inside the model during training, RAG systems dynamically fetch relevant information from external data sources before generating a response.\n",
    "\n",
    "In a typical RAG pipeline, documents such as PDFs, text files, or web pages are first loaded and converted into a structured format. These documents are then split into smaller chunks to ensure efficient processing. Each chunk is transformed into a numerical representation called an embedding, which captures the semantic meaning of the text.\n",
    "\n",
    "When a user asks a question, the query is also converted into an embedding and compared against the stored document embeddings using a vector similarity search. The most relevant chunks are retrieved and passed to a language model as additional context. This allows the model to produce more accurate, up-to-date, and context-aware answers.\n",
    "\n",
    "RAG is especially useful in applications like chatbots, question-answering systems, and enterprise knowledge assistants, where factual accuracy and domain-specific information are critical. By grounding model responses in retrieved documents, RAG helps reduce hallucinations and improves trustworthiness.\"\"\"\n",
    "\n",
    "}\n",
    "\n",
    "for file_path, content in sample_texts.items():\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "179fb548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../data/text_files/doc1.txt'}, page_content='RAG Intoduction\\n    Retrieval-Augmented Generation (RAG) is a technique used to enhance large language models by combining text generation with information retrieval. Instead of relying only on the knowledge stored inside the model during training, RAG systems dynamically fetch relevant information from external data sources before generating a response.\\n\\nIn a typical RAG pipeline, documents such as PDFs, text files, or web pages are first loaded and converted into a structured format. These documents are then split into smaller chunks to ensure efficient processing. Each chunk is transformed into a numerical representation called an embedding, which captures the semantic meaning of the text.\\n\\nWhen a user asks a question, the query is also converted into an embedding and compared against the stored document embeddings using a vector similarity search. The most relevant chunks are retrieved and passed to a language model as additional context. This allows the model to produce more accurate, up-to-date, and context-aware answers.\\n\\nRAG is especially useful in applications like chatbots, question-answering systems, and enterprise knowledge assistants, where factual accuracy and domain-specific information are critical. By grounding model responses in retrieved documents, RAG helps reduce hallucinations and improves trustworthiness.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##TextLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader= TextLoader(\"../data/text_files/doc1.txt\", encoding=\"utf-8\")\n",
    "documents=loader.load()\n",
    "documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db4c8dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '..\\\\data\\\\text_files\\\\doc1.txt'}, page_content='RAG Intoduction\\n    Retrieval-Augmented Generation (RAG) is a technique used to enhance large language models by combining text generation with information retrieval. Instead of relying only on the knowledge stored inside the model during training, RAG systems dynamically fetch relevant information from external data sources before generating a response.\\n\\nIn a typical RAG pipeline, documents such as PDFs, text files, or web pages are first loaded and converted into a structured format. These documents are then split into smaller chunks to ensure efficient processing. Each chunk is transformed into a numerical representation called an embedding, which captures the semantic meaning of the text.\\n\\nWhen a user asks a question, the query is also converted into an embedding and compared against the stored document embeddings using a vector similarity search. The most relevant chunks are retrieved and passed to a language model as additional context. This allows the model to produce more accurate, up-to-date, and context-aware answers.\\n\\nRAG is especially useful in applications like chatbots, question-answering systems, and enterprise knowledge assistants, where factual accuracy and domain-specific information are critical. By grounding model responses in retrieved documents, RAG helps reduce hallucinations and improves trustworthiness.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## directory loader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "dir_loader= DirectoryLoader(\"../data/text_files\", \n",
    "glob=\"*.txt\",  #Pattern to match files\n",
    "loader_cls=TextLoader, #Loader class to use\n",
    "loader_kwargs={\"encoding\":\"utf-8\"},#Loader specific arguments\n",
    "show_progress=False\n",
    ")\n",
    "\n",
    "documents=dir_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee4cdaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF files created successfully.\n"
     ]
    }
   ],
   "source": [
    "from reportlab.lib.pagesizes import LETTER\n",
    "from reportlab.pdfgen import canvas\n",
    "import os\n",
    "\n",
    "# Ensure folder exists\n",
    "os.makedirs(\"../data/pdf_files\", exist_ok=True)\n",
    "\n",
    "# ---------- PDF 1 ----------\n",
    "pdf1_path = \"../data/pdf_files/rag_intro.pdf\"\n",
    "c1 = canvas.Canvas(pdf1_path, pagesize=LETTER)\n",
    "\n",
    "c1.drawString(72, 750, \"Introduction to Retrieval-Augmented Generation (RAG)\")\n",
    "c1.drawString(72, 720, \"RAG combines information retrieval with text generation.\")\n",
    "c1.drawString(72, 690, \"It improves factual accuracy by grounding LLMs in documents.\")\n",
    "\n",
    "c1.save()\n",
    "\n",
    "# ---------- PDF 2 ----------\n",
    "pdf2_path = \"../data/pdf_files/rag_pipeline.pdf\"\n",
    "c2 = canvas.Canvas(pdf2_path, pagesize=LETTER)\n",
    "\n",
    "c2.drawString(72, 750, \"RAG Pipeline Overview\")\n",
    "c2.drawString(72, 720, \"1. Load documents (PDFs, text files, etc.)\")\n",
    "c2.drawString(72, 690, \"2. Chunk documents\")\n",
    "c2.drawString(72, 660, \"3. Generate embeddings\")\n",
    "c2.drawString(72, 630, \"4. Retrieve relevant chunks for a query\")\n",
    "\n",
    "c2.save()\n",
    "\n",
    "print(\"PDF files created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9395152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': 'anonymous', 'creationdate': '2026-01-24T10:59:30+05:00', 'source': '..\\\\data\\\\pdf_files\\\\rag_intro.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\rag_intro.pdf', 'total_pages': 1, 'format': 'PDF 1.3', 'title': 'untitled', 'author': 'anonymous', 'subject': 'unspecified', 'keywords': '', 'moddate': '2026-01-24T10:59:30+05:00', 'trapped': '', 'modDate': \"D:20260124105930+05'00'\", 'creationDate': \"D:20260124105930+05'00'\", 'page': 0}, page_content='Introduction to Retrieval-Augmented Generation (RAG)\\nRAG combines information retrieval with text generation.\\nIt improves factual accuracy by grounding LLMs in documents.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': 'anonymous', 'creationdate': '2026-01-24T10:59:30+05:00', 'source': '..\\\\data\\\\pdf_files\\\\rag_pipeline.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\rag_pipeline.pdf', 'total_pages': 1, 'format': 'PDF 1.3', 'title': 'untitled', 'author': 'anonymous', 'subject': 'unspecified', 'keywords': '', 'moddate': '2026-01-24T10:59:30+05:00', 'trapped': '', 'modDate': \"D:20260124105930+05'00'\", 'creationDate': \"D:20260124105930+05'00'\", 'page': 0}, page_content='RAG Pipeline Overview\\n1. Load documents (PDFs, text files, etc.)\\n2. Chunk documents\\n3. Generate embeddings\\n4. Retrieve relevant chunks for a query')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader,PyMuPDFLoader\n",
    "\n",
    "dir_loader= DirectoryLoader(\n",
    "    \"../data/pdf_files\", \n",
    "    glob=\"*.pdf\",  #Pattern to match files\n",
    "    loader_cls=PyMuPDFLoader, #Loader class to use\n",
    "    #loader_cls=PyPDFLoader,\n",
    "   \n",
    "    show_progress=False\n",
    ")\n",
    "\n",
    "pdf_documents=dir_loader.load()\n",
    "pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c0bed4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "print(type(pdf_documents))\n",
    "print(type(pdf_documents[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a80d29",
   "metadata": {},
   "source": [
    "### embedding and vectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78bd971",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
